import streamlit as st
import pandas as pd
from typing import TypedDict, List, Dict, Any, Optional
from typing_extensions import NotRequired
import re
from datetime import datetime
from collections import Counter
import json
import os

# LangGraph
from langgraph.graph import StateGraph, END

# HuggingFace
from transformers import pipeline

# ë¬¸ì¥ ë¶„ë¦¬
from kss import split_sentences

# OpenAI
from openai import OpenAI

# Tavily
from tavily import TavilyClient

# ============================================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================================
st.set_page_config(
    page_title="ğŸ­ ê°ì • ë¶„ì„ ë©€í‹°ì—ì´ì „íŠ¸",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ============================================================================
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'emotion_classifier' not in st.session_state:
    st.session_state.emotion_classifier = None
if 'analysis_result' not in st.session_state:
    st.session_state.analysis_result = None

# ============================================================================
# State ì •ì˜
# ============================================================================
class AppState(TypedDict, total=False):
    # ì…ë ¥ ë°ì´í„°
    raw_input: str
    text: str
    messages: List[Dict[str, Any]]
    
    # EmotionAgent ê²°ê³¼
    emotion_df: NotRequired[pd.DataFrame]
    emotion_summary: Dict[str, Any]
    
    # ë³‘ë ¬ ì‹¤í–‰ Agent ê²°ê³¼ë“¤
    insight_text: str  # InsightAgent
    statistical_summary: Dict[str, Any]  # SummaryAgent
    extracted_keywords: List[str]  # KeywordExtractorAgent
    
    # ContentAgent ê²°ê³¼
    content_query: str
    content_recos: List[Dict[str, str]]
    
    # Aggregator ìµœì¢… ê²°ê³¼
    final_report: Dict[str, Any]
    
    # ì§„í–‰ ìƒí™© ì¶”ì 
    completed_agents: List[str]

# ============================================================================
# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
# ============================================================================
@st.cache_resource
def load_emotion_model():
    """í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”©"""
    try:
        classifier = pipeline(
            "text-classification",
            model="Seonghaa/korean-emotion-classifier-roberta",
            top_k=3
        )
        return classifier
    except Exception as e:
        st.error(f"ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

def init_openai_client(api_key: str):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if api_key:
        return OpenAI(api_key=api_key)
    return None

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================
def parse_kakao_txt(text: str) -> List[Dict[str, Any]]:
    """ì¹´ì¹´ì˜¤í†¡ í…ìŠ¤íŠ¸ íŒŒì‹±"""
    messages = []
    
    pattern1 = r'(\d{4}ë…„\s+\d{1,2}ì›”\s+\d{1,2}ì¼.*?),\s*(.+?)\s*:\s*(.+)'
    pattern2 = r'(\d{4}\.\d{1,2}\.\d{1,2}\s+\d{1,2}:\d{2}:\d{2}\s+[AP]M),\s*(.+?)\s*:\s*(.+)'
    
    lines = text.strip().split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        match = re.match(pattern1, line)
        if not match:
            match = re.match(pattern2, line)
        
        if match:
            datetime_str = match.group(1).strip()
            speaker = match.group(2).strip()
            msg_text = match.group(3).strip()
            
            messages.append({
                "speaker": speaker,
                "datetime": datetime_str,
                "text": msg_text,
                "emotions": []
            })
    
    return messages

def analyze_emotion(text: str, classifier) -> List[Dict[str, Any]]:
    """ê°ì • ë¶„ì„"""
    if classifier is None:
        return [{"label": "ì¤‘ë¦½", "score": 0.5}]
    
    try:
        results = classifier(text)[0]
        return results
    except Exception as e:
        return [{"label": "ì¤‘ë¦½", "score": 0.5}]

# ============================================================================
# Agent í•¨ìˆ˜ë“¤
# ============================================================================

def preprocessor_agent(state: AppState, classifier) -> AppState:
    """ì „ì²˜ë¦¬ ì—ì´ì „íŠ¸ - CSV íŒŒì‹± ë° êµ¬ì¡°í™”"""
    raw_input = state.get("raw_input", "")
    text = raw_input
    messages = parse_kakao_txt(text)
    
    if not messages:
        sentences = split_sentences(text)
        messages = [
            {
                "speaker": "Unknown",
                "datetime": datetime.now().strftime("%Yë…„ %mì›” %dì¼"),
                "text": sent,
                "emotions": []
            }
            for sent in sentences if sent.strip()
        ]
    
    state["text"] = text
    state["messages"] = messages
    state["completed_agents"] = ["preprocessor"]
    
    return state

def emotion_agent(state: AppState, classifier) -> AppState:
    """ê°ì • ë¶„ì„ ì—ì´ì „íŠ¸"""
    messages = state.get("messages", [])
    
    for msg in messages:
        text = msg.get("text", "")
        emotions = analyze_emotion(text, classifier)
        msg["emotions"] = emotions
    
    rows = []
    for msg in messages:
        speaker = msg.get("speaker", "Unknown")
        datetime_str = msg.get("datetime", "")
        text = msg.get("text", "")
        emotions = msg.get("emotions", [])
        
        if emotions:
            top_emotion = max(emotions, key=lambda x: x["score"])
            emotion_label = top_emotion["label"]
            emotion_score = top_emotion["score"]
        else:
            emotion_label = "ì¤‘ë¦½"
            emotion_score = 0.5
        
        rows.append({
            "speaker": speaker,
            "datetime": datetime_str,
            "text": text,
            "emotion": emotion_label,
            "score": emotion_score,
            "text_length": len(text)
        })
    
    emotion_df = pd.DataFrame(rows)
    
    total_msgs = len(messages)
    emotion_counts = Counter([row["emotion"] for row in rows])
    emotion_ratios = {k: v/total_msgs for k, v in emotion_counts.items()}
    dominant_label = max(emotion_counts, key=emotion_counts.get) if emotion_counts else "ì¤‘ë¦½"
    
    speaker_analysis = {}
    speakers = emotion_df["speaker"].unique()
    
    for speaker in speakers:
        speaker_msgs = emotion_df[emotion_df["speaker"] == speaker]
        speaker_emotions = Counter(speaker_msgs["emotion"].tolist())
        speaker_dominant = max(speaker_emotions, key=speaker_emotions.get) if speaker_emotions else "ì¤‘ë¦½"
        avg_score = speaker_msgs["score"].mean()
        
        speaker_analysis[speaker] = {
            "message_count": len(speaker_msgs),
            "dominant_emotion": speaker_dominant,
            "emotion_distribution": dict(speaker_emotions),
            "avg_score": avg_score
        }
    
    emotion_summary = {
        "total_msgs": total_msgs,
        "counts": dict(emotion_counts),
        "ratios": emotion_ratios,
        "dominant_label": dominant_label,
        "speaker_analysis": speaker_analysis
    }
    
    state["messages"] = messages
    state["emotion_df"] = emotion_df
    state["emotion_summary"] = emotion_summary
    state["completed_agents"] = state.get("completed_agents", []) + ["emotion"]
    
    return state

def insight_agent(state: AppState, openai_client) -> AppState:
    """ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—ì´ì „íŠ¸ (ë³‘ë ¬ ì‹¤í–‰ 1)"""
    emotion_summary = state.get("emotion_summary", {})
    emotion_df = state.get("emotion_df", pd.DataFrame())
    speaker_analysis = emotion_summary.get("speaker_analysis", {})
    
    if not openai_client:
        state["insight_text"] = "âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        state["completed_agents"] = state.get("completed_agents", []) + ["insight"]
        return state
    
    try:
        prompt = f"""ë‹¤ìŒì€ ëŒ€í™” ì°¸ì—¬ìë“¤ì˜ ê°ì • ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

ğŸ“Š ì „ì²´ ì§‘ê³„:
- ì´ ë©”ì‹œì§€: {emotion_summary.get('total_msgs', 0)}ê°œ
- ì£¼ìš” ê°ì •: {emotion_summary.get('dominant_label', 'ì¤‘ë¦½')}
- ê°ì • ë¶„í¬: {emotion_summary.get('counts', {})}

ğŸ‘¥ í™”ìë³„ ë¶„ì„:
{json.dumps(speaker_analysis, ensure_ascii=False, indent=2)}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **500ì ì´ë‚´**ë¡œ í•µì‹¬ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì•„ë˜ 1~3ë²ˆì„ ê°ê° í•œ ì„¸ ë¬¸ì¥ì”©, ì´ 9ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.

1. ì „ë°˜ì ì¸ ê°ì • íŒ¨í„´:
   - ëŒ€í™”ì˜ ì‹œì‘â€“ì¤‘ê°„â€“ë íë¦„ì„ ê¸°ì¤€ìœ¼ë¡œ, ì–´ë–¤ ê°ì •ì´ ì–´ë–»ê²Œ ë³€í™”í–ˆëŠ”ì§€ ì„œìˆ í•˜ì„¸ìš”.
   - ë‹¨ìˆœíˆ 'ê¸°ì¨ì´ ë§ë‹¤'ê°€ ì•„ë‹ˆë¼, ì˜ˆë¥¼ ë“¤ì–´ 'ì´ˆë°˜ì—” í”¼ë¡œì™€ ê±±ì •ì´ ë‘ë“œëŸ¬ì§€ì§€ë§Œ, ì¤‘ê°„ ì´í›„ ì„œë¡œì˜ ìœ„ë¡œë¡œ ë¶„ìœ„ê¸°ê°€ ì ì  ì•ˆì •ë©ë‹ˆë‹¤'ì²˜ëŸ¼ **ê°ì • ì „í™˜**ì´ ë“œëŸ¬ë‚˜ê²Œ ì¨ì£¼ì„¸ìš”.

2. í™”ì ê°„ ê´€ê³„ íŠ¹ì„±:
   - ë‘ í™”ìì˜ ì—­í•  ì°¨ì´(ì˜ˆ: í•œìª½ì€ ê³ ë¯¼ì„ í„¸ì–´ë†“ê³ , ë‹¤ë¥¸ ìª½ì€ ìœ„ë¡œÂ·ì¡°ì–¸ ì¤‘ì‹¬ì¸ì§€), ê³µê°Â·ì§€ì§€Â·ê°ˆë“± ì—¬ë¶€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê´€ê³„ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.
   - 'ì‚¬ì´ê°€ ì¢‹ë‹¤' ìˆ˜ì¤€ì´ ì•„ë‹ˆë¼, ì˜ˆë¥¼ ë“¤ì–´ 'AëŠ” ì†”ì§í•˜ê²Œ í˜ë“  ê°ì •ì„ í„¸ì–´ë†“ê³ , BëŠ” ì´ë¥¼ ì§„ì§€í•˜ê²Œ ë°›ì•„ë“¤ì´ë©° ê³µê°í•´ì£¼ëŠ” ê´€ê³„ì…ë‹ˆë‹¤'ì²˜ëŸ¼ **ìƒí˜¸ì‘ìš©ì˜ íŠ¹ì§•**ì´ ë“œëŸ¬ë‚˜ê²Œ ì¨ì£¼ì„¸ìš”.

3. ê°„ë‹¨í•œ ì¡°ì–¸:
   - 1â€“2ë²ˆì—ì„œ ë“œëŸ¬ë‚œ ê°ì • íë¦„ê³¼ ê´€ê³„ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ, ë‘ ì‚¬ëŒì´ ê°ì •ì„ ë” ê±´ê°•í•˜ê²Œ ë‚˜ëˆ„ê±°ë‚˜ ê´€ê³„ë¥¼ ìœ ì§€Â·ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë  ë§Œí•œ **êµ¬ì²´ì ì¸ í–‰ë™ í•œ ê°€ì§€**ë¥¼ ì œì•ˆí•˜ì„¸ìš”.
   - ë°”ë¡œ ì‹¤ì²œ ê°€ëŠ¥í•œ í˜„ì‹¤ì ì¸ ì¡°ì–¸ìœ¼ë¡œ, ë¬¸ì¥ ëì— ì´ëª¨ì§€ 1ê°œë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”.

í˜•ì‹ ê·œì¹™:
- ê° í•­ëª©ì€ '1. ë¬¸ì¥', '2. ë¬¸ì¥', '3. ë¬¸ì¥' í˜•íƒœë¡œ, í•œ ì¤„ì— 3ë¬¸ì¥ì”©ë§Œ ì ìŠµë‹ˆë‹¤.
- ì´ 9ë¬¸ì¥ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
- í•œêµ­ì–´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
- ì „ì²´ ë¶„ëŸ‰ì€ 500ìë¥¼ ë„˜ê¸°ì§€ ë§ˆì„¸ìš”.
"""
        
        response = openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê°ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=1000,
            temperature=0.7
        )
        
        state["insight_text"] = response.choices[0].message.content
    
    except Exception as e:
        state["insight_text"] = f"âš ï¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}"
    
    state["completed_agents"] = state.get("completed_agents", []) + ["insight"]
    return state

def summary_agent(state: AppState) -> AppState:
    """í†µê³„ ìš”ì•½ ì—ì´ì „íŠ¸ (ë³‘ë ¬ ì‹¤í–‰ 2)"""
    emotion_df = state.get("emotion_df", pd.DataFrame())
    messages = state.get("messages", [])
    
    if emotion_df.empty:
        state["completed_agents"] = state.get("completed_agents", []) + ["summary"]
        return state
    
    # ì‹œê°„ëŒ€ë³„ ë¶„ì„
    time_distribution = {}
    for msg in messages:
        datetime_str = msg.get("datetime", "")
        if "AM" in datetime_str or "ì˜¤ì „" in datetime_str:
            time_period = "ì˜¤ì „"
        elif "PM" in datetime_str or "ì˜¤í›„" in datetime_str:
            time_period = "ì˜¤í›„"
        else:
            time_period = "ê¸°íƒ€"
        
        time_distribution[time_period] = time_distribution.get(time_period, 0) + 1
    
    # ë©”ì‹œì§€ ê¸¸ì´ í†µê³„
    avg_length = emotion_df["text_length"].mean()
    max_length = emotion_df["text_length"].max()
    min_length = emotion_df["text_length"].min()
    
    # í™”ì ì°¸ì—¬ë„
    speaker_participation = emotion_df["speaker"].value_counts().to_dict()
    
    statistical_summary = {
        "time_distribution": time_distribution,
        "message_length": {
            "average": round(avg_length, 2),
            "max": max_length,
            "min": min_length
        },
        "speaker_participation": speaker_participation,
        "total_speakers": len(emotion_df["speaker"].unique())
    }
    
    state["statistical_summary"] = statistical_summary
    state["completed_agents"] = state.get("completed_agents", []) + ["summary"]
    
    return state

def keyword_extractor_agent(state: AppState) -> AppState:
    """í‚¤ì›Œë“œ ì¶”ì¶œ ì—ì´ì „íŠ¸ (ë³‘ë ¬ ì‹¤í–‰ 3)"""
    messages = state.get("messages", [])
    emotion_summary = state.get("emotion_summary", {})
    
    # ëª¨ë“  ë©”ì‹œì§€ í…ìŠ¤íŠ¸ ê²°í•©
    all_text = " ".join([msg.get("text", "") for msg in messages])
    
    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
    words = re.findall(r'\b\w{2,}\b', all_text)
    word_counts = Counter(words)
    
    # ë¶ˆìš©ì–´ ì œê±°
    stopwords = ['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë“¤', 'ì•ˆ', 'ì•Š', 'ë„¤', 'ì˜ˆ', 'ë•Œ', 'ê±°', 'ë‚˜', 'ë„ˆ', 'ìš°ë¦¬', 'ìˆ', 'ì—†', 'í•˜']
    filtered_words = {word: count for word, count in word_counts.items() if word not in stopwords}
    
    # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ
    top_keywords = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:10]
    extracted_keywords = [word for word, _ in top_keywords]
    
    # ê°ì • ê¸°ë°˜ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
    dominant = emotion_summary.get("dominant_label", "ì¤‘ë¦½")
    dominant_lower = dominant.lower()
    
    if any(word in dominant_lower for word in ["ìŠ¬í””", "ìš°ìš¸", "sad", "ë¶€ì •"]):
        content_query = "ìš°ìš¸ ìŠ¬í”” ê°ì • ê´€ë¦¬ ì‹¬ë¦¬ ìƒë‹´"
    elif any(word in dominant_lower for word in ["ë¶ˆì•ˆ", "ê±±ì •", "anxious"]):
        content_query = "ë¶ˆì•ˆ ê±±ì • ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë§ˆìŒì±™ê¹€"
    elif any(word in dominant_lower for word in ["ë¶„ë…¸", "í™”", "angry"]):
        content_query = "ë¶„ë…¸ ì¡°ì ˆ ê°ˆë“± í•´ê²° ì˜ì‚¬ì†Œí†µ"
    elif any(word in dominant_lower for word in ["ê¸°ì¨", "í–‰ë³µ", "ê¸ì •", "positive", "happy"]):
        content_query = "í–‰ë³µ ê¸ì • ê°ì • ìœ ì§€ ê´€ê³„ ê°œì„ "
    else:
        content_query = "ê°ì • ê´€ë¦¬ ì‹¬ë¦¬ ê±´ê°• ìê¸°ê³„ë°œ"
    
    state["extracted_keywords"] = extracted_keywords
    state["content_query"] = content_query
    state["completed_agents"] = state.get("completed_agents", []) + ["keyword"]
    
    return state

def content_agent(state: AppState, tavily_api_key: str) -> AppState:
    """ì½˜í…ì¸  ì¶”ì²œ ì—ì´ì „íŠ¸"""
    content_query = state.get("content_query", "ê°ì • ê´€ë¦¬")
    
    if not tavily_api_key:
        state["content_recos"] = []
        state["completed_agents"] = state.get("completed_agents", []) + ["content"]
        return state
    
    try:
        client = TavilyClient(api_key=tavily_api_key)
        response = client.search(
            query=content_query,
            search_depth="basic",
            max_results=5
        )
        
        results = []
        for item in response.get('results', []):
            url = item.get('url', '')
            if 'youtube.com' in url or 'youtu.be' in url:
                content_type = "video"
            elif any(domain in url for domain in ['news', 'article', 'blog']):
                content_type = "article"
            else:
                content_type = "content"
            
            results.append({
                "type": content_type,
                "title": item.get('title', 'No title'),
                "url": url,
                "snippet": item.get('content', '')[:150]
            })
        
        state["content_recos"] = results
    
    except Exception as e:
        state["content_recos"] = []
    
    state["completed_agents"] = state.get("completed_agents", []) + ["content"]
    return state

def aggregator_agent(state: AppState, openai_client) -> AppState:
    """ìµœì¢… í†µí•© ì—ì´ì „íŠ¸ - ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•© ì •ë¦¬"""
    
    emotion_summary = state.get("emotion_summary", {})
    statistical_summary = state.get("statistical_summary", {})
    insight_text = state.get("insight_text", "")
    keywords = state.get("extracted_keywords", [])
    content_recos = state.get("content_recos", [])
    
    # í†µí•© ë¦¬í¬íŠ¸ ìƒì„±
    final_report = {
        "overview": {
            "total_messages": emotion_summary.get("total_msgs", 0),
            "total_speakers": statistical_summary.get("total_speakers", 0),
            "dominant_emotion": emotion_summary.get("dominant_label", "ì¤‘ë¦½"),
            "time_distribution": statistical_summary.get("time_distribution", {})
        },
        "emotion_analysis": emotion_summary,
        "statistics": statistical_summary,
        "keywords": keywords[:5],  # ìƒìœ„ 5ê°œ
        "insight": insight_text,
        "recommendations": content_recos[:3]  # ìƒìœ„ 3ê°œ
    }
    
    # OpenAIë¡œ ìµœì¢… ìš”ì•½ ìƒì„± (ì„ íƒ)
    if openai_client:
        try:
            prompt = f"""ë‹¤ìŒì€ ëŒ€í™” ë¶„ì„ì˜ ëª¨ë“  ê²°ê³¼ì…ë‹ˆë‹¤:

ğŸ“Š **ê¸°ë³¸ ì •ë³´**
- ì „ì²´ ë©”ì‹œì§€: {final_report['overview']['total_messages']}ê°œ
- ì°¸ì—¬ì: {final_report['overview']['total_speakers']}ëª…
- ì£¼ìš” ê°ì •: {final_report['overview']['dominant_emotion']}

ğŸ”‘ **í•µì‹¬ í‚¤ì›Œë“œ**
{', '.join(keywords[:5])}

ğŸ’¡ **ì‹¬ë¦¬ ë¶„ì„**
{insight_text}

ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **300ì ì´ë‚´**ë¡œ í•µì‹¬ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ëŒ€í™”ì˜ ì£¼ìš” íŠ¹ì§• (2ì¤„)
2. ê°ì •ì  í•µì‹¬ (2ì¤„)
íŠ¹íˆ ê°ì •ì  í•µì‹¬ì—ëŠ” ì‹œê°„ì— ë”°ë¥¸ ìƒí™©ì„ ë°˜ì˜í•´ì„œ ì–´ë–»ê²Œ ê´€ê³„ë‚˜ ìƒí™©ì´ ë‹¬ë¼ì¡ŒëŠ”ì§€, ìµœì¢…ì ìœ¼ë¡œëŠ” ì–´ë–»ê²Œ ë§ˆë¬´ë¦¬ê°€ ëëŠ”ì§€ ìì„¸í•˜ê²Œ ì‘ì„±í•´ì¤˜.
3. í•œ ì¤„ ì¡°ì–¸ (2ì¤„)

ì´ëª¨ì§€ì™€ í•¨ê»˜ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
            
            response = openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=700,
                temperature=0.7
            )
            
            final_report["summary"] = response.choices[0].message.content
        
        except Exception as e:
            final_report["summary"] = f"âš ï¸ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}"
    else:
        # OpenAI ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ìš”ì•½
        final_report["summary"] = f"""
ğŸ“Š **ë¶„ì„ ìš”ì•½**

ì´ {final_report['overview']['total_messages']}ê°œì˜ ë©”ì‹œì§€ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.
{final_report['overview']['total_speakers']}ëª…ì´ ëŒ€í™”ì— ì°¸ì—¬í–ˆìœ¼ë©°, ì£¼ìš” ê°ì •ì€ **{final_report['overview']['dominant_emotion']}**ì…ë‹ˆë‹¤.

í•µì‹¬ í‚¤ì›Œë“œ: {', '.join(keywords[:3])}
        """
    
    state["final_report"] = final_report
    state["completed_agents"] = state.get("completed_agents", []) + ["aggregator"]
    
    return state

# ============================================================================
# ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def run_analysis(text: str, openai_key: str, tavily_key: str, classifier):
    """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    openai_client = init_openai_client(openai_key) if openai_key else None
    
    initial_state = {"raw_input": text}
    
    # 1. Preprocessor
    with st.status("ğŸ”„ PreprocessorAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
        st.write("CSV íŒŒì‹± ë° ì „ì²˜ë¦¬ ì¤‘...")
        state = preprocessor_agent(initial_state, classifier)
        st.write(f"âœ… {len(state['messages'])}ê°œ ë©”ì‹œì§€ íŒŒì‹± ì™„ë£Œ")
        status.update(label="âœ… Preprocessor ì™„ë£Œ", state="complete")
    
    # 2. Emotion Agent
    with st.status("ğŸŸ¢ EmotionAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
        st.write("ê°ì • ë¶„ì„ ì¤‘...")
        state = emotion_agent(state, classifier)
        st.write(f"âœ… ì£¼ìš” ê°ì •: {state['emotion_summary']['dominant_label']}")
        status.update(label="âœ… EmotionAgent ì™„ë£Œ", state="complete")
    
    # 3. ë³‘ë ¬ ì‹¤í–‰ (Insight + Summary + Keyword)
    with st.status("âš¡ ë³‘ë ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...", expanded=True) as status:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.write("ğŸŸ¡ InsightAgent...")
            state = insight_agent(state, openai_client)
            st.write("âœ… ì™„ë£Œ")
        
        with col2:
            st.write("ğŸ”µ SummaryAgent...")
            state = summary_agent(state)
            st.write("âœ… ì™„ë£Œ")
        
        with col3:
            st.write("ğŸŸ£ KeywordAgent...")
            state = keyword_extractor_agent(state)
            st.write("âœ… ì™„ë£Œ")
        
        status.update(label="âœ… ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ", state="complete")
    
    # 4. Content Agent
    if tavily_key:
        with st.status("ğŸ”´ ContentAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
            st.write("ì½˜í…ì¸  ì¶”ì²œ ì¤‘...")
            state = content_agent(state, tavily_key)
            st.write(f"âœ… {len(state.get('content_recos', []))}ê°œ ì¶”ì²œ ì™„ë£Œ")
            status.update(label="âœ… ContentAgent ì™„ë£Œ", state="complete")
    
    # 5. Aggregator (ìµœì¢… í†µí•©)
    with st.status("ğŸ“Š AggregatorAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
        st.write("ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        state = aggregator_agent(state, openai_client)
        st.write("âœ… í†µí•© ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        status.update(label="âœ… Aggregator ì™„ë£Œ", state="complete")
    
    return state

# ============================================================================
# Streamlit UI
# ============================================================================
def main():
    st.title("ğŸ­ ê°ì • ë¶„ì„ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ")
    st.markdown("### ë³‘ë ¬ ì‹¤í–‰ ì•„í‚¤í…ì²˜ + Aggregator")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        openai_key = st.text_input("OpenAI API Key", type="password", 
                                   help="ì„ íƒ - ì¸ì‚¬ì´íŠ¸ ë° í†µí•© ìš”ì•½ ìƒì„±")
        tavily_key = st.text_input("Tavily API Key", type="password",
                                   help="ì„ íƒ - ì½˜í…ì¸  ì¶”ì²œ")
        
        st.divider()
        
        # ëª¨ë¸ ë¡œë”©
        if not st.session_state.models_loaded:
            if st.button("ğŸš€ ëª¨ë¸ ë¡œë”©", use_container_width=True):
                with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘... (ì•½ 1-2ë¶„ ì†Œìš”)"):
                    st.session_state.emotion_classifier = load_emotion_model()
                    if st.session_state.emotion_classifier:
                        st.session_state.models_loaded = True
                        st.success("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                        st.rerun()
                    else:
                        st.error("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        else:
            st.success("âœ… ëª¨ë¸ ë¡œë”©ë¨")
            if st.button("ğŸ”„ ëª¨ë¸ ì¬ë¡œë”©", use_container_width=True):
                st.session_state.models_loaded = False
                st.session_state.emotion_classifier = None
                st.rerun()
        
        st.divider()
        st.caption("ğŸ—ï¸ ì•„í‚¤í…ì²˜:")
        st.code("""
Preprocessor
    â†“
Emotion
    â†“
â”Œâ”€â”€â”€â”¼â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
I   S   K    (ë³‘ë ¬)
â””â”€â”€â”€â”¼â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
    â†“
Content
    â†“
Aggregator ğŸ“Š
        """)
    
    # ë©”ì¸ ì˜ì—­
    if not st.session_state.models_loaded:
        st.warning("âš ï¸ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ë¡œë”©í•´ì£¼ì„¸ìš”.")
        return
    
    st.header("ğŸ“ ëŒ€í™” ì…ë ¥")
    
    # ì…ë ¥ ë°©ì‹ ì„ íƒ
    input_method = st.radio(
        "ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["ğŸ“ CSV íŒŒì¼ ì—…ë¡œë“œ", "âœï¸ ì§ì ‘ ì…ë ¥"],
        horizontal=True
    )
    
    input_text = ""
    
    if input_method == "ğŸ“ CSV íŒŒì¼ ì—…ë¡œë“œ":
        st.info("ğŸ’¡ ì¹´ì¹´ì˜¤í†¡ â†’ ëŒ€í™”ë°© â†’ ì„¤ì •(â‰¡) â†’ 'ëŒ€í™” ë‚´ë³´ë‚´ê¸°' â†’ CSV íŒŒì¼ ì €ì¥")
        st.caption("ğŸ“‹ í•„ìˆ˜ ì»¬ëŸ¼: `date`, `user`, `message`")
        
        uploaded_file = st.file_uploader(
            "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” CSV íŒŒì¼ ì—…ë¡œë“œ",
            type=['csv'],
            help="date, user, message ì»¬ëŸ¼ì´ ìˆëŠ” CSV íŒŒì¼"
        )
        
        if uploaded_file is not None:
            try:
                try:
                    df = pd.read_csv(uploaded_file, encoding='utf-8')
                except:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding='cp949')
                
                st.success(f"âœ… CSV íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {uploaded_file.name}")
                
                with st.expander("ğŸ“Š CSV ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°"):
                    st.dataframe(df.head(10), use_container_width=True)
                    st.caption(f"ì´ {len(df)}ê°œ í–‰")
                
                df.columns = df.columns.str.lower().str.strip()
                
                if 'date' in df.columns and 'user' in df.columns and 'message' in df.columns:
                    lines = []
                    for _, row in df.iterrows():
                        date = str(row['date']).strip()
                        user = str(row['user']).strip()
                        message = str(row['message']).strip()
                        
                        if message and message != 'nan':
                            lines.append(f"{date}, {user} : {message}")
                    
                    input_text = '\n'.join(lines)
                    st.info(f"âœ… {len(lines)}ê°œ ë©”ì‹œì§€ ë³€í™˜ ì™„ë£Œ")
                    
                    with st.expander("ğŸ“ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 5ì¤„)"):
                        st.text('\n'.join(lines[:5]))
                        if len(lines) > 5:
                            st.caption(f"... ì™¸ {len(lines) - 5}ì¤„")
                else:
                    st.error(f"âŒ CSV íŒŒì¼ì— 'date', 'user', 'message' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.\n\ní˜„ì¬ ì»¬ëŸ¼: {list(df.columns)}")
                    
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
    
    else:  # ì§ì ‘ ì…ë ¥
        sample_text = """2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:30, ê¹€ì² ìˆ˜ : ì˜¤ëŠ˜ ì •ë§ í˜ë“  í•˜ë£¨ì˜€ì–´
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:31, ì´ì˜í¬ : ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:32, ê¹€ì² ìˆ˜ : íšŒì‚¬ì—ì„œ ì¼ì´ ë„ˆë¬´ ë§ì•„ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:33, ì´ì˜í¬ : í˜ë“¤ê² ë‹¤ ã… ã…  ë„ˆë¬´ ê±±ì •ë˜ë„¤
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:35, ê¹€ì² ìˆ˜ : ë¶ˆì•ˆí•˜ê³  ìš°ìš¸í•´... ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´"""
        
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("ğŸ“‹ ìƒ˜í”Œ ë°ì´í„°", use_container_width=True):
                st.session_state.input_text = sample_text
        
        input_text = st.text_area(
            "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë˜ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            value=st.session_state.get('input_text', ''),
            height=250,
            help="ì¹´ì¹´ì˜¤í†¡ í˜•ì‹: YYYYë…„ MMì›” DDì¼ ì‹œê°„, ì´ë¦„ : ë©”ì‹œì§€"
        )
    
    # ë¶„ì„ ì‹¤í–‰
    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        if not input_text.strip():
            st.error("ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê±°ë‚˜ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return
        
        # ë¶„ì„ ì‹¤í–‰
        result = run_analysis(
            input_text, 
            openai_key, 
            tavily_key,
            st.session_state.emotion_classifier
        )
        
        st.session_state.analysis_result = result
        st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
    
    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.analysis_result:
        result = st.session_state.analysis_result
        final_report = result.get("final_report", {})
        
        st.divider()
        
        # ğŸ“Š Aggregatorê°€ ë§Œë“  ìµœì¢… ìš”ì•½ (ì œì¼ ìœ„ì— í‘œì‹œ)
        st.header("ğŸ“Š ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸")
        
        summary = final_report.get("summary", "")
        if summary:
            st.markdown(summary)
        
        # ì£¼ìš” ì§€í‘œ ì¹´ë“œ
        overview = final_report.get("overview", {})
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ“ ì „ì²´ ë©”ì‹œì§€", overview.get("total_messages", 0))
        with col2:
            st.metric("ğŸ‘¥ ì°¸ì—¬ì", overview.get("total_speakers", 0))
        with col3:
            st.metric("ğŸ˜Š ì£¼ìš” ê°ì •", overview.get("dominant_emotion", "ì¤‘ë¦½"))
        with col4:
            time_dist = overview.get("time_distribution", {})
            main_time = max(time_dist, key=time_dist.get) if time_dist else "N/A"
            st.metric("â° ì£¼ìš” ì‹œê°„ëŒ€", main_time)
        
        st.divider()
        
        # íƒ­ìœ¼ë¡œ ìƒì„¸ ê²°ê³¼ êµ¬ë¶„
        tabs = st.tabs(["ğŸ“ˆ ê°ì • ë¶„ì„", "ğŸ’¡ ì¸ì‚¬ì´íŠ¸", "ğŸ“Š í†µê³„", "ğŸ”‘ í‚¤ì›Œë“œ", "ğŸ¬ ì¶”ì²œ ì½˜í…ì¸ "])
        
        # ğŸ“ˆ ê°ì • ë¶„ì„ íƒ­
        with tabs[0]:
            emotion_summary = result.get("emotion_summary", {})
            emotion_df = result.get("emotion_df", pd.DataFrame())
            
            st.subheader("ğŸ“Š ê°ì • ë¶„í¬")
            emotion_counts = emotion_summary.get("counts", {})
            if emotion_counts:
                chart_df = pd.DataFrame(list(emotion_counts.items()), 
                                       columns=["ê°ì •", "ê°œìˆ˜"])
                st.bar_chart(chart_df.set_index("ê°ì •"))
            
            st.subheader("ğŸ‘¥ í™”ìë³„ ë¶„ì„")
            speaker_analysis = emotion_summary.get("speaker_analysis", {})
            for speaker, data in speaker_analysis.items():
                with st.expander(f"**{speaker}** - {data['message_count']}ê°œ ë©”ì‹œì§€"):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ì£¼ìš” ê°ì •", data['dominant_emotion'])
                    with col2:
                        st.metric("í‰ê·  ì ìˆ˜", f"{data['avg_score']:.3f}")
                    st.write(f"**ê°ì • ë¶„í¬:** {data['emotion_distribution']}")
            
            st.subheader("ğŸ“‹ ë©”ì‹œì§€ë³„ ìƒì„¸")
            if not emotion_df.empty:
                st.dataframe(emotion_df, use_container_width=True)
        
        # ğŸ’¡ ì¸ì‚¬ì´íŠ¸ íƒ­
        with tabs[1]:
            insight_text = result.get("insight_text", "")
            if insight_text:
                st.markdown(insight_text)
            else:
                st.info("ğŸ’¡ OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ë” ìƒì„¸í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")
        
        # ğŸ“Š í†µê³„ íƒ­
        with tabs[2]:
            statistical_summary = result.get("statistical_summary", {})
            
            st.subheader("â° ì‹œê°„ëŒ€ë³„ ë¶„í¬")
            time_dist = statistical_summary.get("time_distribution", {})
            if time_dist:
                time_df = pd.DataFrame(list(time_dist.items()), 
                                      columns=["ì‹œê°„ëŒ€", "ë©”ì‹œì§€ ìˆ˜"])
                st.bar_chart(time_df.set_index("ì‹œê°„ëŒ€"))
            
            st.subheader("ğŸ“ ë©”ì‹œì§€ ê¸¸ì´ í†µê³„")
            msg_length = statistical_summary.get("message_length", {})
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("í‰ê·  ê¸¸ì´", f"{msg_length.get('average', 0)}ì")
            with col2:
                st.metric("ìµœëŒ€ ê¸¸ì´", f"{msg_length.get('max', 0)}ì")
            with col3:
                st.metric("ìµœì†Œ ê¸¸ì´", f"{msg_length.get('min', 0)}ì")
            
            st.subheader("ğŸ‘¥ í™”ìë³„ ì°¸ì—¬ë„")
            speaker_participation = statistical_summary.get("speaker_participation", {})
            if speaker_participation:
                part_df = pd.DataFrame(list(speaker_participation.items()), 
                                      columns=["í™”ì", "ë©”ì‹œì§€ ìˆ˜"])
                st.bar_chart(part_df.set_index("í™”ì"))
        
        # ğŸ”‘ í‚¤ì›Œë“œ íƒ­
        with tabs[3]:
            keywords = result.get("extracted_keywords", [])
            
            st.subheader("ğŸ”‘ ì¶”ì¶œëœ í•µì‹¬ í‚¤ì›Œë“œ")
            if keywords:
                # í‚¤ì›Œë“œë¥¼ ì»¬ëŸ¼ìœ¼ë¡œ ë‚˜ëˆ„ì–´ í‘œì‹œ (ê°€ë…ì„± í–¥ìƒ)
                cols = st.columns(3)
                for idx, kw in enumerate(keywords[:10]):
                    col_idx = idx % 3
                    with cols[col_idx]:
                        st.markdown(f"""
                        <div style='
                            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                            color: white;
                            padding: 12px 20px;
                            margin: 8px 0;
                            border-radius: 20px;
                            text-align: center;
                            font-weight: bold;
                            font-size: 16px;
                            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
                        '>
                            #{kw}
                        </div>
                        """, unsafe_allow_html=True)
            else:
                st.info("í‚¤ì›Œë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            st.divider()
            
            content_query = result.get("content_query", "")
            if content_query:
                st.subheader("ğŸ” ì½˜í…ì¸  ê²€ìƒ‰ ì¿¼ë¦¬")
                st.info(f"**{content_query}**")
                st.caption("ğŸ‘† ì´ í‚¤ì›Œë“œë¡œ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤")
        
        # ğŸ¬ ì¶”ì²œ ì½˜í…ì¸  íƒ­
        with tabs[4]:
            content_recos = result.get("content_recos", [])
            
            if content_recos:
                st.subheader(f"ğŸ¬ ì¶”ì²œ ì½˜í…ì¸  ({len(content_recos)}ê°œ)")
                
                for idx, item in enumerate(content_recos, 1):
                    with st.container():
                        col1, col2 = st.columns([1, 11])
                        
                        with col1:
                            if item["type"] == "video":
                                st.write("ğŸ¬")
                            elif item["type"] == "article":
                                st.write("ğŸ“°")
                            else:
                                st.write("ğŸ”—")
                        
                        with col2:
                            st.markdown(f"**{idx}. [{item['title']}]({item['url']})**")
                            st.caption(item['snippet'])
                        
                        st.divider()
            else:
                st.warning("ğŸ’¡ Tavily API í‚¤ë¥¼ ì…ë ¥í•˜ë©´ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()