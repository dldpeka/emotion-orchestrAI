import streamlit as st
import pandas as pd
from typing import TypedDict, List, Dict, Any, Optional
from typing_extensions import NotRequired
import re
from datetime import datetime
from collections import Counter
import json
import os

# LangGraph
from langgraph.graph import StateGraph, END

# HuggingFace
from transformers import pipeline

# ë¬¸ì¥ ë¶„ë¦¬
from kss import split_sentences

# OpenAI
from openai import OpenAI

# Tavily
from tavily import TavilyClient

# ============================================================================
# í˜ì´ì§€ ì„¤ì •
# ============================================================================
st.set_page_config(
    page_title="ğŸ­ ê°ì • ë¶„ì„ ë©€í‹°ì—ì´ì „íŠ¸",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============================================================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ============================================================================
if 'models_loaded' not in st.session_state:
    st.session_state.models_loaded = False
if 'emotion_classifier' not in st.session_state:
    st.session_state.emotion_classifier = None
if 'openai_client' not in st.session_state:
    st.session_state.openai_client = None
if 'analysis_result' not in st.session_state:
    st.session_state.analysis_result = None

# ============================================================================
# State ì •ì˜
# ============================================================================
class AppState(TypedDict, total=False):
    raw_input: str
    input_type: str
    analysis_mode: str
    messages: List[Dict[str, Any]]
    text: str
    required_agents: List[str]
    emotion_df: NotRequired[pd.DataFrame]
    agg_result: Dict[str, Any]
    insight_text: str
    content_query: str
    content_recos: List[Dict[str, str]]
    completed_agents: List[str]

# ============================================================================
# ëª¨ë¸ ë¡œë”© í•¨ìˆ˜
# ============================================================================
@st.cache_resource
def load_emotion_model():
    """í•œêµ­ì–´ ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”©"""
    try:
        classifier = pipeline(
            "text-classification",
            model="Seonghaa/korean-emotion-classifier-roberta",
            top_k=3
        )
        return classifier
    except Exception as e:
        st.error(f"ê°ì • ë¶„ì„ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None

def init_openai_client(api_key: str):
    """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    if api_key:
        return OpenAI(api_key=api_key)
    return None

# ============================================================================
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
# ============================================================================
def parse_kakao_txt(text: str) -> List[Dict[str, Any]]:
    """ì¹´ì¹´ì˜¤í†¡ í…ìŠ¤íŠ¸ íŒŒì‹±"""
    messages = []
    pattern = r'(\d{4}ë…„\s+\d{1,2}ì›”\s+\d{1,2}ì¼.*?),\s*(.+?)\s*:\s*(.+)'
    
    lines = text.strip().split('\n')
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        match = re.match(pattern, line)
        if match:
            datetime_str = match.group(1).strip()
            speaker = match.group(2).strip()
            msg_text = match.group(3).strip()
            
            messages.append({
                "speaker": speaker,
                "datetime": datetime_str,
                "text": msg_text,
                "emotions": []
            })
    
    return messages

def analyze_emotion(text: str, classifier) -> List[Dict[str, Any]]:
    """ê°ì • ë¶„ì„"""
    if classifier is None:
        return [{"label": "ì¤‘ë¦½", "score": 0.5}]
    
    try:
        results = classifier(text)[0]
        return results
    except Exception as e:
        return [{"label": "ì¤‘ë¦½", "score": 0.5}]

def llm_orchestrator_decision(analysis_mode: str, text_preview: str, client) -> List[str]:
    """LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì„ íƒ"""
    if analysis_mode == "emotion_only":
        return ["emotion"]
    elif analysis_mode == "insight_only":
        return ["emotion", "insight"]
    elif analysis_mode == "full":
        return ["emotion", "insight", "content"]
    
    if not client:
        return ["emotion", "insight"]
    
    try:
        prompt = f"""ë‹¹ì‹ ì€ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ Orchestratorì…ë‹ˆë‹¤.
ì…ë ¥ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì—ì´ì „íŠ¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”.

**ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:**
1. emotion - ê°ì • ë¶„ì„ ì—ì´ì „íŠ¸
2. insight - ì¸ì‚¬ì´íŠ¸ ìƒì„± ì—ì´ì „íŠ¸
3. content - ì½˜í…ì¸  ì¶”ì²œ ì—ì´ì „íŠ¸

**ì…ë ¥ ë°ì´í„°:**
{text_preview[:500]}...

í•„ìš”í•œ ì—ì´ì „íŠ¸ ëª©ë¡ì„ JSON ë°°ì—´ë¡œë§Œ ë‹µí•˜ì„¸ìš” (ì˜ˆ: ["emotion", "insight"]):"""
        
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì‘ì—… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=500,
            temperature=0.3
        )
        
        result_text = response.choices[0].message.content.strip()
        result_text = result_text.replace("```json", "").replace("```", "").strip()
        required = json.loads(result_text)
        return required
    
    except Exception as e:
        st.warning(f"Orchestrator íŒë‹¨ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
        return ["emotion", "insight"]

def generate_llm_insight(agg_result: Dict[str, Any], emotion_df: pd.DataFrame, 
                        speaker_analysis: Dict[str, Any], client) -> str:
    """LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    if not client:
        return "âš ï¸ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤."
    
    try:
        emotion_summary = emotion_df.head(20).to_string()
        
        prompt = f"""ë‹¤ìŒì€ ëŒ€í™” ì°¸ì—¬ìë“¤ì˜ ê°ì • ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

ğŸ“Š ì „ì²´ ì§‘ê³„:
{json.dumps(agg_result, ensure_ascii=False, indent=2)}

ğŸ‘¥ í™”ìë³„ ë¶„ì„:
{json.dumps(speaker_analysis, ensure_ascii=False, indent=2)}

ğŸ“ ë©”ì‹œì§€ë³„ ìƒì„¸ ë°ì´í„° (ìƒ˜í”Œ):
{emotion_summary}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•œ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

1. **ì „ë°˜ì ì¸ ê°ì • íŒ¨í„´ ë¶„ì„**
2. **í™”ìë³„ ê°ì • íŠ¹ì„± ë° ê´€ê³„ ì—­í•™ ë¶„ì„**
3. **ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë³€í™”**
4. **ì‹¬ë¦¬ì  ì¡°ì–¸ ë° ê´€ê³„ ê°œì„  ì œì•ˆ**

ë”°ëœ»í•˜ê³  ê³µê°ì ì¸ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì‹œê³ , ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ í™œìš©í•´ì£¼ì„¸ìš”."""
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ê°ì • ë¶„ì„ ë° ê´€ê³„ ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2000,
            temperature=0.7
        )
        
        return response.choices[0].message.content
    
    except Exception as e:
        return f"âš ï¸ LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}"

def search_with_tavily(query: str, api_key: str, max_results: int = 5) -> List[Dict[str, str]]:
    """Tavily ê²€ìƒ‰"""
    if not api_key:
        return []
    
    try:
        client = TavilyClient(api_key=api_key)
        response = client.search(
            query=query,
            search_depth="basic",
            max_results=max_results
        )
        
        results = []
        for item in response.get('results', []):
            url = item.get('url', '')
            if 'youtube.com' in url or 'youtu.be' in url:
                content_type = "video"
            elif any(domain in url for domain in ['news', 'article', 'blog']):
                content_type = "article"
            else:
                content_type = "content"
            
            results.append({
                "type": content_type,
                "title": item.get('title', 'No title'),
                "url": url,
                "snippet": item.get('content', '')[:150]
            })
        
        return results
    
    except Exception as e:
        st.error(f"Tavily ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# ============================================================================
# ì—ì´ì „íŠ¸ í•¨ìˆ˜ë“¤
# ============================================================================
def aggregator_agent(state: AppState, classifier, openai_client) -> AppState:
    """ì „ì²˜ë¦¬ ë° Orchestrator"""
    raw_input = state.get("raw_input", "")
    analysis_mode = state.get("analysis_mode", "auto")
    
    text = raw_input
    messages = parse_kakao_txt(text)
    
    if not messages:
        sentences = split_sentences(text)
        messages = [
            {
                "speaker": "Unknown",
                "datetime": datetime.now().strftime("%Yë…„ %mì›” %dì¼"),
                "text": sent,
                "emotions": []
            }
            for sent in sentences if sent.strip()
        ]
    
    required_agents = llm_orchestrator_decision(analysis_mode, text, openai_client)
    
    state["text"] = text
    state["messages"] = messages
    state["required_agents"] = required_agents
    state["completed_agents"] = []
    
    return state

def emotion_agent(state: AppState, classifier) -> AppState:
    """ê°ì • ë¶„ì„"""
    messages = state.get("messages", [])
    
    for msg in messages:
        text = msg.get("text", "")
        emotions = analyze_emotion(text, classifier)
        msg["emotions"] = emotions
    
    rows = []
    for msg in messages:
        speaker = msg.get("speaker", "Unknown")
        datetime_str = msg.get("datetime", "")
        text = msg.get("text", "")
        emotions = msg.get("emotions", [])
        
        if emotions:
            top_emotion = max(emotions, key=lambda x: x["score"])
            emotion_label = top_emotion["label"]
            emotion_score = top_emotion["score"]
        else:
            emotion_label = "ì¤‘ë¦½"
            emotion_score = 0.5
        
        rows.append({
            "speaker": speaker,
            "datetime": datetime_str,
            "text": text,
            "emotion": emotion_label,
            "score": emotion_score,
            "text_length": len(text)
        })
    
    emotion_df = pd.DataFrame(rows)
    
    total_msgs = len(messages)
    emotion_counts = Counter([row["emotion"] for row in rows])
    emotion_ratios = {k: v/total_msgs for k, v in emotion_counts.items()}
    dominant_label = max(emotion_counts, key=emotion_counts.get) if emotion_counts else "ì¤‘ë¦½"
    
    speaker_analysis = {}
    speakers = emotion_df["speaker"].unique()
    
    for speaker in speakers:
        speaker_msgs = emotion_df[emotion_df["speaker"] == speaker]
        speaker_emotions = Counter(speaker_msgs["emotion"].tolist())
        speaker_dominant = max(speaker_emotions, key=speaker_emotions.get) if speaker_emotions else "ì¤‘ë¦½"
        avg_score = speaker_msgs["score"].mean()
        
        speaker_analysis[speaker] = {
            "message_count": len(speaker_msgs),
            "dominant_emotion": speaker_dominant,
            "emotion_distribution": dict(speaker_emotions),
            "avg_score": avg_score
        }
    
    agg_result = {
        "total_msgs": total_msgs,
        "counts": dict(emotion_counts),
        "ratios": emotion_ratios,
        "dominant_label": dominant_label,
        "speaker_analysis": speaker_analysis
    }
    
    state["messages"] = messages
    state["emotion_df"] = emotion_df
    state["agg_result"] = agg_result
    state["completed_agents"] = state.get("completed_agents", []) + ["emotion"]
    
    return state

def insight_agent(state: AppState, openai_client) -> AppState:
    """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
    agg_result = state.get("agg_result", {})
    emotion_df = state.get("emotion_df", pd.DataFrame())
    speaker_analysis = agg_result.get("speaker_analysis", {})
    
    insight_text = generate_llm_insight(agg_result, emotion_df, speaker_analysis, openai_client)
    
    dominant = agg_result.get("dominant_label", "ì¤‘ë¦½")
    dominant_lower = dominant.lower()
    
    if any(word in dominant_lower for word in ["ìŠ¬í””", "ìš°ìš¸", "sad", "ë¶€ì •"]):
        content_query = "ìš°ìš¸ ìŠ¬í”” ê°ì • ê´€ë¦¬ ì‹¬ë¦¬ ìƒë‹´"
    elif any(word in dominant_lower for word in ["ë¶ˆì•ˆ", "ê±±ì •", "anxious"]):
        content_query = "ë¶ˆì•ˆ ê±±ì • ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë§ˆìŒì±™ê¹€"
    elif any(word in dominant_lower for word in ["ë¶„ë…¸", "í™”", "angry"]):
        content_query = "ë¶„ë…¸ ì¡°ì ˆ ê°ˆë“± í•´ê²° ì˜ì‚¬ì†Œí†µ"
    elif any(word in dominant_lower for word in ["ê¸°ì¨", "í–‰ë³µ", "ê¸ì •", "positive", "happy"]):
        content_query = "í–‰ë³µ ê¸ì • ê°ì • ìœ ì§€ ê´€ê³„ ê°œì„ "
    else:
        content_query = "ê°ì • ê´€ë¦¬ ì‹¬ë¦¬ ê±´ê°• ìê¸°ê³„ë°œ"
    
    state["insight_text"] = insight_text
    state["content_query"] = content_query
    state["completed_agents"] = state.get("completed_agents", []) + ["insight"]
    
    return state

def content_recommender_agent(state: AppState, tavily_api_key: str) -> AppState:
    """ì½˜í…ì¸  ì¶”ì²œ"""
    content_query = state.get("content_query", "ê°ì • ê´€ë¦¬")
    content_recos = search_with_tavily(content_query, tavily_api_key, max_results=5)
    
    state["content_recos"] = content_recos
    state["completed_agents"] = state.get("completed_agents", []) + ["content"]
    
    return state

# ============================================================================
# ë¼ìš°íŒ… í•¨ìˆ˜ë“¤
# ============================================================================
def route_after_aggregator(state: AppState) -> str:
    required = state.get("required_agents", [])
    if "emotion" in required:
        return "emotion"
    elif "insight" in required:
        return "insight"
    elif "content" in required:
        return "content"
    else:
        return "end"

def route_after_emotion(state: AppState) -> str:
    required = state.get("required_agents", [])
    completed = state.get("completed_agents", [])
    remaining = [a for a in required if a not in completed]
    
    if "insight" in remaining:
        return "insight"
    elif "content" in remaining:
        return "content"
    else:
        return "end"

def route_after_insight(state: AppState) -> str:
    required = state.get("required_agents", [])
    completed = state.get("completed_agents", [])
    remaining = [a for a in required if a not in completed]
    
    if "content" in remaining:
        return "content"
    else:
        return "end"

# ============================================================================
# ë¶„ì„ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================
def run_analysis(text: str, analysis_mode: str, openai_key: str, tavily_key: str, classifier):
    """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    
    # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    openai_client = init_openai_client(openai_key) if openai_key else None
    
    # ì´ˆê¸° ìƒíƒœ ìƒì„±
    initial_state = {
        "raw_input": text,
        "analysis_mode": analysis_mode
    }
    
    # 1. Aggregator ì‹¤í–‰
    with st.status("ğŸ­ Aggregator ì‹¤í–‰ ì¤‘...", expanded=True) as status:
        st.write("ì…ë ¥ ì „ì²˜ë¦¬ ë° ì—ì´ì „íŠ¸ ì„ íƒ ì¤‘...")
        state = aggregator_agent(initial_state, classifier, openai_client)
        st.write(f"âœ… {len(state['messages'])}ê°œ ë©”ì‹œì§€ íŒŒì‹± ì™„ë£Œ")
        st.write(f"ğŸ¯ ì‹¤í–‰í•  ì—ì´ì „íŠ¸: {state['required_agents']}")
        status.update(label="âœ… Aggregator ì™„ë£Œ", state="complete")
    
    required = state.get("required_agents", [])
    
    # 2. Emotion Agent ì‹¤í–‰
    if "emotion" in required:
        with st.status("ğŸŸ¢ EmotionAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
            st.write("ê°ì • ë¶„ì„ ì¤‘...")
            state = emotion_agent(state, classifier)
            st.write(f"âœ… ê°ì • ë¶„ì„ ì™„ë£Œ - ì£¼ìš” ê°ì •: {state['agg_result']['dominant_label']}")
            status.update(label="âœ… EmotionAgent ì™„ë£Œ", state="complete")
    
    # 3. Insight Agent ì‹¤í–‰
    if "insight" in required:
        with st.status("ğŸŸ¡ InsightAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
            st.write("ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘...")
            state = insight_agent(state, openai_client)
            st.write("âœ… ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ")
            st.write(f"ğŸ” ì½˜í…ì¸  ê²€ìƒ‰ í‚¤ì›Œë“œ: {state['content_query']}")
            status.update(label="âœ… InsightAgent ì™„ë£Œ", state="complete")
    
    # 4. Content Agent ì‹¤í–‰
    if "content" in required:
        with st.status("ğŸ”´ ContentAgent ì‹¤í–‰ ì¤‘...", expanded=True) as status:
            st.write("ì½˜í…ì¸  ì¶”ì²œ ì¤‘...")
            state = content_recommender_agent(state, tavily_key)
            st.write(f"âœ… {len(state.get('content_recos', []))}ê°œ ì½˜í…ì¸  ì¶”ì²œ ì™„ë£Œ")
            status.update(label="âœ… ContentAgent ì™„ë£Œ", state="complete")
    
    return state

# ============================================================================
# Streamlit UI
# ============================================================================
def main():
    st.title("ğŸ­ ê°ì • ë¶„ì„ ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ")
    st.markdown("### LangGraph + HuggingFace + OpenAI")
    
    # ì‚¬ì´ë“œë°” - API í‚¤ ì„¤ì •
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        openai_key = st.text_input("OpenAI API Key", type="password", 
                                   help="í•„ìˆ˜ - Orchestratorì™€ ì¸ì‚¬ì´íŠ¸ ìƒì„±ì— ì‚¬ìš©ë©ë‹ˆë‹¤")
        tavily_key = st.text_input("Tavily API Key", type="password",
                                   help="ì„ íƒ - ì½˜í…ì¸  ì¶”ì²œì— ì‚¬ìš©ë©ë‹ˆë‹¤")
        
        st.divider()
        
        st.header("ğŸ¯ ë¶„ì„ ëª¨ë“œ")
        analysis_mode = st.radio(
            "ëª¨ë“œ ì„ íƒ",
            ["auto", "full", "emotion_only", "insight_only"],
            help="""
            - auto: LLMì´ ìë™ìœ¼ë¡œ íŒë‹¨
            - full: ì „ì²´ ë¶„ì„ (ê°ì • + ì¸ì‚¬ì´íŠ¸ + ì½˜í…ì¸ )
            - emotion_only: ê°ì • ë¶„ì„ë§Œ
            - insight_only: ê´€ê³„ ì¸ì‚¬ì´íŠ¸ë§Œ
            """
        )
        
        st.divider()
        
        # ëª¨ë¸ ë¡œë”©
        if not st.session_state.models_loaded:
            if st.button("ğŸš€ ëª¨ë¸ ë¡œë”©", use_container_width=True):
                with st.spinner("ëª¨ë¸ ë¡œë”© ì¤‘..."):
                    st.session_state.emotion_classifier = load_emotion_model()
                    st.session_state.models_loaded = True
                    st.success("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
                    st.rerun()
        else:
            st.success("âœ… ëª¨ë¸ ë¡œë”©ë¨")
    
    # ë©”ì¸ ì˜ì—­
    if not st.session_state.models_loaded:
        st.warning("âš ï¸ ë¨¼ì € ì‚¬ì´ë“œë°”ì—ì„œ ëª¨ë¸ì„ ë¡œë”©í•´ì£¼ì„¸ìš”.")
        return
    
    # ì…ë ¥ ì˜ì—­
    st.header("ğŸ“ ëŒ€í™” ì…ë ¥")
    
    # ì…ë ¥ ë°©ì‹ ì„ íƒ
    input_method = st.radio(
        "ì…ë ¥ ë°©ì‹ ì„ íƒ",
        ["ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", "âœï¸ ì§ì ‘ ì…ë ¥"],
        horizontal=True
    )
    
    input_text = ""
    
    if input_method == "ğŸ“ íŒŒì¼ ì—…ë¡œë“œ":
        st.info("ğŸ’¡ ì¹´ì¹´ì˜¤í†¡ â†’ ëŒ€í™”ë°© â†’ ì„¤ì •(â‰¡) â†’ 'ëŒ€í™” ë‚´ë³´ë‚´ê¸°' â†’ TXT íŒŒì¼ ì €ì¥")
        
        uploaded_file = st.file_uploader(
            "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë‚´ë³´ë‚´ê¸° íŒŒì¼ (.txt)",
            type=['txt'],
            help="ì¹´ì¹´ì˜¤í†¡ì—ì„œ ë‚´ë³´ë‚¸ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
        )
        
        if uploaded_file is not None:
            # íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ì²˜ë¦¬)
            try:
                input_text = uploaded_file.read().decode('utf-8')
                st.success(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {uploaded_file.name}")
                
                # ë¯¸ë¦¬ë³´ê¸°
                with st.expander("ğŸ“„ íŒŒì¼ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 10ì¤„)"):
                    preview_lines = input_text.split('\n')[:10]
                    st.text('\n'.join(preview_lines))
                    if len(input_text.split('\n')) > 10:
                        st.caption(f"... ì™¸ {len(input_text.split('\n')) - 10}ì¤„")
            except UnicodeDecodeError:
                try:
                    uploaded_file.seek(0)
                    input_text = uploaded_file.read().decode('cp949')
                    st.success(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {uploaded_file.name} (CP949 ì¸ì½”ë”©)")
                except:
                    st.error("âŒ íŒŒì¼ ì¸ì½”ë”© ì˜¤ë¥˜. UTF-8 ë˜ëŠ” ANSI í˜•ì‹ì˜ íŒŒì¼ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
    
    else:  # ì§ì ‘ ì…ë ¥
        # ìƒ˜í”Œ ë°ì´í„°
        sample_text = """2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:30, ê¹€ì² ìˆ˜ : ì˜¤ëŠ˜ ì •ë§ í˜ë“  í•˜ë£¨ì˜€ì–´
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:31, ì´ì˜í¬ : ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:32, ê¹€ì² ìˆ˜ : íšŒì‚¬ì—ì„œ ì¼ì´ ë„ˆë¬´ ë§ì•„ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:33, ì´ì˜í¬ : í˜ë“¤ê² ë‹¤ ã… ã…  ë„ˆë¬´ ê±±ì •ë˜ë„¤
2024ë…„ 12ì›” 11ì¼ ì˜¤í›„ 2:35, ê¹€ì² ìˆ˜ : ë¶ˆì•ˆí•˜ê³  ìš°ìš¸í•´... ì–´ë–»ê²Œ í•´ì•¼ í• ì§€ ëª¨ë¥´ê² ì–´"""
        
        col1, col2 = st.columns([3, 1])
        with col2:
            if st.button("ğŸ“‹ ìƒ˜í”Œ ë°ì´í„°", use_container_width=True):
                st.session_state.input_text = sample_text
        
        input_text = st.text_area(
            "ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë˜ëŠ” ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”",
            value=st.session_state.get('input_text', ''),
            height=250,
            help="ì¹´ì¹´ì˜¤í†¡ í˜•ì‹: YYYYë…„ MMì›” DDì¼ ì‹œê°„, ì´ë¦„ : ë©”ì‹œì§€"
        )
    
    # ë¶„ì„ ì‹¤í–‰
    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        if not input_text.strip():
            st.error("ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
        
        if not openai_key and analysis_mode in ["auto", "full", "insight_only"]:
            st.error("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return
        
        # ë¶„ì„ ì‹¤í–‰
        result = run_analysis(
            input_text, 
            analysis_mode, 
            openai_key, 
            tavily_key,
            st.session_state.emotion_classifier
        )
        
        st.session_state.analysis_result = result
        st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
    
    # ê²°ê³¼ í‘œì‹œ
    if st.session_state.analysis_result:
        result = st.session_state.analysis_result
        
        st.divider()
        st.header("ğŸ“Š ë¶„ì„ ê²°ê³¼")
        
        # ì‹¤í–‰ëœ ì—ì´ì „íŠ¸ í‘œì‹œ
        completed = result.get("completed_agents", [])
        st.info(f"ğŸ¯ ì‹¤í–‰ëœ ì—ì´ì „íŠ¸: {', '.join(completed)}")
        
        # íƒ­ìœ¼ë¡œ ê²°ê³¼ êµ¬ë¶„
        tabs = []
        if "emotion" in completed:
            tabs.append("ğŸ“ˆ ê°ì • ë¶„ì„")
        if "insight" in completed:
            tabs.append("ğŸ’¡ ì¸ì‚¬ì´íŠ¸")
        if "content" in completed:
            tabs.append("ğŸ¬ ì½˜í…ì¸  ì¶”ì²œ")
        
        if tabs:
            tab_objects = st.tabs(tabs)
            tab_idx = 0
            
            # ê°ì • ë¶„ì„ íƒ­
            if "emotion" in completed:
                with tab_objects[tab_idx]:
                    agg = result.get("agg_result", {})
                    emotion_df = result.get("emotion_df", pd.DataFrame())
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ì „ì²´ ë©”ì‹œì§€", agg.get("total_msgs", 0))
                    with col2:
                        st.metric("ì£¼ìš” ê°ì •", agg.get("dominant_label", "ì¤‘ë¦½"))
                    with col3:
                        dominant_ratio = agg.get("ratios", {}).get(agg.get("dominant_label", ""), 0)
                        st.metric("ì£¼ìš” ê°ì • ë¹„ìœ¨", f"{dominant_ratio*100:.1f}%")
                    
                    st.subheader("ğŸ“Š ê°ì • ë¶„í¬")
                    emotion_counts = agg.get("counts", {})
                    if emotion_counts:
                        chart_df = pd.DataFrame(list(emotion_counts.items()), 
                                               columns=["ê°ì •", "ê°œìˆ˜"])
                        st.bar_chart(chart_df.set_index("ê°ì •"))
                    
                    st.subheader("ğŸ‘¥ í™”ìë³„ ë¶„ì„")
                    speaker_analysis = agg.get("speaker_analysis", {})
                    for speaker, data in speaker_analysis.items():
                        with st.expander(f"**{speaker}** - {data['message_count']}ê°œ ë©”ì‹œì§€"):
                            st.write(f"**ì£¼ìš” ê°ì •:** {data['dominant_emotion']}")
                            st.write(f"**í‰ê·  ì ìˆ˜:** {data['avg_score']:.3f}")
                            st.write(f"**ê°ì • ë¶„í¬:** {data['emotion_distribution']}")
                    
                    st.subheader("ğŸ“‹ ë©”ì‹œì§€ë³„ ìƒì„¸")
                    st.dataframe(emotion_df, use_container_width=True)
                
                tab_idx += 1
            
            # ì¸ì‚¬ì´íŠ¸ íƒ­
            if "insight" in completed:
                with tab_objects[tab_idx]:
                    insight_text = result.get("insight_text", "")
                    st.markdown(insight_text)
                    
                    st.divider()
                    st.info(f"ğŸ” **ì½˜í…ì¸  ê²€ìƒ‰ í‚¤ì›Œë“œ:** {result.get('content_query', '')}")
                    st.caption("ğŸ‘† ì´ í‚¤ì›Œë“œê°€ ContentAgentì—ê²Œ ì „ë‹¬ë˜ì–´ ê´€ë ¨ ì½˜í…ì¸ ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤")
                
                tab_idx += 1
            
            # ì½˜í…ì¸  ì¶”ì²œ íƒ­
            if "content" in completed:
                with tab_objects[tab_idx]:
                    content_recos = result.get("content_recos", [])
                    
                    if content_recos:
                        st.write(f"**ì´ {len(content_recos)}ê°œì˜ ì¶”ì²œ ì½˜í…ì¸ **")
                        
                        for idx, item in enumerate(content_recos, 1):
                            with st.container():
                                col1, col2 = st.columns([1, 11])
                                
                                with col1:
                                    if item["type"] == "video":
                                        st.write("ğŸ¬")
                                    elif item["type"] == "article":
                                        st.write("ğŸ“°")
                                    else:
                                        st.write("ğŸ”—")
                                
                                with col2:
                                    st.markdown(f"**{idx}. [{item['title']}]({item['url']})**")
                                    st.caption(item['snippet'])
                                
                                st.divider()
                    else:
                        st.warning("ì¶”ì²œ ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤. Tavily API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()